<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Homework 8 — Probability interpretations, measure theory & Poisson counting process</title>

<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script" async></script>

<style>
:root{
  --bg:#f5f6fa; --card:#ffffff; --text:#172031; --muted:#475569;
  --accent-a:#0ea5e9; --accent-b:#f97316; --radius:12px; --shadow:0 10px 30px rgba(17,24,39,0.06);
  --max-w:980px; --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
}
*{box-sizing:border-box;}
html,body{height:100%;}
body{
  margin:0;
  font-family:"Inter",system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial;
  background:var(--bg);
  color:var(--text);
  -webkit-font-smoothing:antialiased;
  -moz-osx-font-smoothing:grayscale;
  line-height:1.6;
}

.container{max-width:var(--max-w);margin:28px auto;padding:28px;}
.header{display:flex;justify-content:space-between;align-items:center;gap:16px;margin-bottom:18px;}
.brand{display:flex;align-items:center;gap:12px;}
.brand-mark{width:46px;height:46px;border-radius:10px;display:inline-grid;place-items:center;color:white;font-weight:700;background:linear-gradient(135deg,var(--accent-a),var(--accent-b));box-shadow:0 8px 20px rgba(14,165,233,0.08);font-size:16px;}
.brand-title .title{font-weight:700;color:var(--accent-b);font-size:1rem;}
.brand-title .subtitle{font-size:0.85rem;color:var(--muted);}

/* page title */
.page-title{margin:6px 0 12px 0;}
.page-title h1{margin:0;font-size:1.7rem;color:var(--accent-a);}
.page-title p{margin:8px 0 0;color:var(--muted);font-size:1rem;}

/* cards */
.card{background:var(--card);border-radius:var(--radius);box-shadow:var(--shadow);padding:18px;margin-bottom:18px;}
h2{color:var(--accent-b);margin:0 0 10px 0;font-size:1.15rem;}
h3{margin:12px 0 8px 0;color:var(--accent-a);font-size:1rem;}
p,li{color:var(--muted);font-size:1rem;margin:8px 0;}
.math-box{background:#f3f6fb;border-radius:10px;padding:12px;margin:12px 0;border:1px solid #e9f0fb;text-align:center;font-size:1.02rem;}
pre{background:#f3f4f6;padding:10px;border-radius:8px;overflow:auto;font-family:var(--mono);font-size:0.95rem;}
label{display:block;margin-top:8px;font-weight:600;color:var(--text);}
input[type="text"],input[type="number"],textarea{width:100%;padding:8px;border-radius:8px;border:1px solid #e6e9ee;font-size:1rem;box-sizing:border-box;color:var(--text);}
.row{display:flex;gap:10px;margin-top:10px;flex-wrap:wrap;}
.row> *{flex:1 1 160px;}
button{background:var(--accent-b);color:#fff;border:none;padding:10px 12px;border-radius:8px;cursor:pointer;font-weight:600;}
button.ghost{background:#fff;color:var(--accent-a);border:1px solid #e6e9ee;}
.small{font-size:0.95rem;color:var(--muted);}

/* charts */
.grid-2{display:grid;grid-template-columns:1fr 1fr;gap:12px;}
@media(max-width:760px){.grid-2{grid-template-columns:1fr;}}
.canvas-wrap{background:#fff;padding:12px;border-radius:10px;border:1px solid #eef2f6;}
.canvas{width:100%;height:320px;display:block;}

/* code viewer */
.code-toggle{display:flex;gap:10px;align-items:center;margin-top:12px;}
.code-block{display:none;margin-top:12px;background:#fff;padding:12px;border-radius:10px;border:1px solid #eef2f6;white-space:pre-wrap;font-family:var(--mono);}

/* footer */
.footer{margin-top:20px;padding-top:18px;border-top:1px solid #eef2f6;color:var(--muted);text-align:center;font-size:0.95rem;}
</style>
</head>
<body>
  <div class="container">

    <div class="header">
      <a class="brand" href="../index.html">
        <div class="brand-mark">S</div>
        <div class="brand-title">
          <div class="title">Statistics</div>
        </div>
      </a>

      <nav style="display:flex;gap:10px;align-items:center;">
        <a href="../index.html" style="color:var(--muted);text-decoration:none;">Home</a>
        <a href="../homework7/homework7.html" style="color:var(--muted);text-decoration:none;">HW7</a>
      </nav>
    </div>

    <div class="page-title">
      <h1>Homework 8 — Probability foundations & Poisson counting process</h1>
      <p>Interpretations of probability, measure-theoretic connection, axiomatic derivations, and simulation of a counting process approximating a Poisson process.</p>
    </div>

    <!-- Part A: interpretations & measure theory -->
    <section class="card">
      <h2>A — Interpretations of probability & the axiomatic resolution</h2>

      <h3>Common interpretations (short)</h3>
      <p><strong>Classical (Laplace):</strong> probability as ratio of favourable to equally possible outcomes, e.g. fair dice. Works when symmetry / equipossibility are justified.</p>

      <p><strong>Frequentist:</strong> probability as long-run relative frequency of an event after repeated independent trials (empirical). Good for physical repeatable experiments; ties naturally to law of large numbers.</p>

      <p><strong>Bayesian:</strong> probability as degree of belief (subjective) assigned to a proposition, updated via Bayes' rule as evidence arrives. Useful when modeling uncertain knowledge and combining prior information.</p>

      <p><strong>Geometric:</strong> probability defined using lengths, areas, or volumes (e.g., Buffon's needle), effectively a special case of measure-based probability.</p>

      <h3>Why the axiomatic approach helps</h3>
      <p>The Kolmogorov axioms give a single, rigorous framework that all interpretations can be embedded into:</p>
      <div class="math-box">
        \(\begin{aligned}
          &\text{(1) } P(\Omega) = 1,\\
          &\text{(2) } P(A) \ge 0 \text{ for all events } A,\\
          &\text{(3) } \text{If } A_1,A_2,\dots \text{ are disjoint then } P\Big(\bigcup_{i} A_i\Big) = \sum_i P(A_i).
        \end{aligned}\)
      </div>

      <p>These axioms remove contradictions because they do not commit to one philosophical meaning: they simply specify the rules any probability assignment must satisfy. Both frequentist long-run frequencies and Bayesian degrees of belief are modeled as probability measures that satisfy the axioms; geometric constructions are just probability measures built from Lebesgue measure restricted and normalized to an event set.</p>

      <h3>My note</h3>
      <p>Axioms unify the field: once you accept them, you can safely transfer intuition and results across interpretations — e.g., frequentist sampling theory uses the same measure-theoretic tools Bayesians use to compute posterior probabilities; the difference lies primarily in modeling choices (priors, experiment setup), not the mathematics.</p>
    </section>

    <section class="card">
      <h2>B — Probability theory ⇄ Measure theory</h2>

      <p>Measure theory supplies the technical language needed for modern probability. The main objects:</p>
      <ul>
        <li><strong>Sample space</strong> \(\Omega\): the set of all possible outcomes.</li>
        <li><strong>Sigma-algebra</strong> \(\mathcal{F}\): a collection of subsets of \(\Omega\) (events) closed under complement and countable unions. This gives a domain where measure (probability) is defined.</li>
        <li><strong>Probability measure</strong> \(P\): a function \(P:\mathcal{F}\to[0,1]\) satisfying Kolmogorov axioms.</li>
        <li><strong>Measurable function / random variable</strong>: a function \(X:(\Omega,\mathcal{F})\to(\mathbb{R},\mathcal{B})\) (where \(\mathcal{B}\) is Borel sigma-algebra) such that preimages of Borel sets are in \(\mathcal{F}\). This allows us to push the probability measure forward and define distributions of \(X\).</li>
      </ul>

      <p>Benefit: measure theory handles subtle issues (uncountable spaces, continuous distributions, integrals as expectations) and gives a single rigorous way to define expectation, variance, conditional probability (via Radon–Nikodym derivatives), convergence modes (almost sure, in probability, L^p), and limit theorems.</p>

      <h3>Short definitions</h3>
      <div class="math-box">
        \( \text{Expectation: } E[X] = \int_\Omega X(\omega)\,dP(\omega). \)
      </div>
      <p>Integration replaces sums when the variable is continuous or when the sample space is uncountable.</p>
    </section>

    <section class="card">
      <h2>C — Derivations from axioms</h2>

      <h3>Subadditivity</h3>
      <p>For any events \(A,B\) we have \(P(A\cup B)\le P(A) + P(B)\). Using disjoint decomposition:</p>
      <div class="math-box">
        \( P(A\cup B) = P(A) + P(B\setminus A) \le P(A) + P(B). \)
      </div>
      <p>For a countable collection \(\{A_i\}_{i=1}^\infty\) we can write the union as a disjoint union of sets \(B_1 = A_1\), \(B_2 = A_2\setminus A_1\), \(B_3 = A_3\setminus (A_1\cup A_2)\), etc., and apply countable additivity to get:</p>
      <div class="math-box">
        \( P\Big(\bigcup_{i=1}^\infty A_i\Big) = \sum_{i=1}^\infty P(B_i) \le \sum_{i=1}^\infty P(A_i). \)
      </div>

      <h3>Inclusion–Exclusion principle (finite form)</h3>
      <p>For two sets:</p>
      <div class="math-box">
        \( P(A\cup B) = P(A) + P(B) - P(A\cap B). \)
      </div>
      <p>For three sets:</p>
      <div class="math-box">
        \( P(A\cup B\cup C) = P(A)+P(B)+P(C) - P(A\cap B) - P(A\cap C) - P(B\cap C) + P(A\cap B\cap C). \)
      </div>
      <p>The formula alternates signs; it follows from repeated application of union decomposition and additivity. Inclusion–exclusion is especially powerful to compute probabilities of unions when intersections can be calculated.</p>
    </section>

    <!-- Part D: simulation -->
    <section class="card">
      <h2>D — Simulation: discrete approximation of a Poisson counting process</h2>

      <h3>Problem & method</h3>
      <p>We simulate events on time interval \([0,T]\). Events occur independently and uniformly with constant average rate \(\lambda\) (events per unit time). The standard continuous-time model for this is the <strong>Poisson process</strong> with rate \(\lambda\):</p>
      <ul>
        <li>Number of events in interval length \(t\) is Poisson distributed with mean \(\lambda t\).</li>
        <li>Independent increments: counts on disjoint intervals are independent.</li>
        <li>Interarrival times are i.i.d. \(\mathrm{Exponential}(\lambda)\).</li>
      </ul>

      <p><strong>Discrete approximation (Bernoulli grid):</strong> divide \([0,T]\) into \(n\) equal subintervals of length \(T/n\). In each subinterval generate an event with probability \(\lambda T / n\) (assuming \(\lambda T / n \le 1\); choose n large enough). This is equivalent to performing \(n\) independent Bernoulli trials and summing the successes. For large n this Binomial\((n,\lambda T/n)\) distribution approaches Poisson\((\lambda T)\).</p>

      <div class="math-box">
        \(\text{If }K_n \sim \mathrm{Binomial}\!\left(n,\frac{\lambda T}{n}\right)\text{ then } K_n \xrightarrow[n\to\infty]{d} \mathrm{Poisson}(\lambda T).\)
      </div>

      <h3>Interpretation of \(\lambda\)</h3>
      <p>\(\lambda\) is the average event rate: expected number of events per unit time. Over interval \(T\), expected count is \(\lambda T\). In the discrete approx each small interval has expected count \(\lambda T / n\).</p>
    </section>

    <section class="card">
      <h2>E — Interactive approximation & analysis</h2>

      <p class="small">Set parameters and run the simulation. The discrete approximation performs many Bernoulli trials per run; the histogram of counts across many runs should match the Poisson(\(\lambda T\)) pmf as \(n\) and the number of simulation runs grow.</p>

      <div class="grid-2">
        <div>
          <label>Time interval T</label>
          <input id="T" type="number" step="0.1" value="1">
          <label>Rate λ (events per unit time)</label>
          <input id="lambda" type="number" step="0.1" value="4">
          <label>Subintervals n (discretization)</label>
          <input id="n" type="number" value="5000">
        </div>
        <div>
          <label>Simulation runs</label>
          <input id="runs" type="number" value="2000">
          <label>Show sample path events? (1 = yes)</label>
          <input id="showPath" type="number" value="1" min="0" max="1">
          <div style="margin-top:10px;display:flex;gap:8px;flex-wrap:wrap;">
            <button id="runBtn">Run simulation</button>
            <button id="clearBtn" class="ghost">Clear</button>
            <button id="showCodeBtn" class="ghost">Show/Hide JS</button>
          </div>
        </div>
      </div>

      <div style="margin-top:14px;">
        <div class="canvas-wrap" style="margin-bottom:12px;"><canvas id="pathChart" class="canvas"></canvas></div>
        <div class="canvas-wrap"><canvas id="countHist" class="canvas"></canvas></div>
      </div>

      <div class="code-block" id="codeBlock" aria-hidden="true"><pre id="sourceArea"><code>loading...</code></pre></div>

      <p style="margin-top:12px;" class="small"><strong>What to check:</strong> empirical mean of counts ≈ λT; empirical distribution of counts ≈ Poisson(λT) (compare pmf); as n increases, discretization bias diminishes; interarrival empirical histogram (from constructed event times) approximates Exponential(λ).</p>
    </section>

    <div class="footer">&copy; 2025 Javid Farzaliyev — La Sapienza University</div>

  </div>

<script src="script.js"></script>
</body>
</html>
